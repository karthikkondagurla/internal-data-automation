# Internal Data Automation

**A production-grade ETL pipeline for financial market data ingestion, processing, and reporting.**

---

## ğŸ“– About

The **Internal Data Automation** system is a robust Python-based pipeline designed to automate the daily workflow of validatng, ingesting, cleaning, and storing financial data. It is built with a focus on reliability, observability, and security, making it suitable for enterprise-grade deployment on AWS.

## âœ¨ Key Features

- **ğŸ›¡ï¸ Production-Ready**: Enforces strict validation, immutable infrastructure patterns, and fail-safe execution.
- **ğŸ³ Containerized**: Fully dockerized application ensuring consistency across Dev, Test, and Prod environments.
- **â˜ï¸ Cloud-Native**:
  - **S3 Integration**: Automated archiving of reports and raw data.
  - **CloudWatch Logs**: Real-time log streaming for production auditing.
- **ğŸ”„ Automated**: Includes scripts for idempotent execution and Cron scheduling.
- **ğŸ”’ Secure**: Zero-trust credential management via environment variables.

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+ (for local development)
- AWS Account (for production features)

### 1. Configuration
Create a `.env` file in the root directory:
```bash
ALPHA_VANTAGE_API_KEY="your_key"
NEWS_API_KEY="your_key"
```

### 2. Run with Docker (Recommended)
Build and run the container in one go:

```bash
# Build
docker build -t internal-data-automation .

# Run (Mounts local volumes for data persistence)
docker run --rm \
  --env-file .env \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/reports:/app/reports" \
  -v "$(pwd)/logs:/app/logs" \
  internal-data-automation
```

### 3. Run Locally
```bash
pip install -r requirements.txt
python run_pipeline.py
```

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    Sources[External APIs] -->|Ingest| Raw(Raw JSON)
    Raw -->|Process| Clean(Cleaned Objects)
    Clean -->|Store| DB[(SQLite/RDS)]
    DB -->|Report| Output[CSV & Summary]
    
    subgraph Production Pipeline
    Output -->|Archive| S3(AWS S3)
    Logs -->|Stream| BW(CloudWatch)
    end
```

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ config.yaml             # App configuration (timeouts, paths)
â”œâ”€â”€ deployment_guide.md     # ğŸ“˜ AWS EC2 Deployment Guide
â”œâ”€â”€ scheduling.md           # â²ï¸ Cron Scheduling Guide
â”œâ”€â”€ Dockerfile              # Production Docker image definition
â”œâ”€â”€ run_pipeline.py         # Main execution entry point
â”œâ”€â”€ scripts/                # Operational scripts (e.g., cron wrappers)
â””â”€â”€ internal_data_automation/
    â”œâ”€â”€ ingestion/          # API Clients with retry logic
    â”œâ”€â”€ processing/         # Data cleaning & normalization
    â”œâ”€â”€ storage/            # Database abstraction
    â””â”€â”€ reporting/          # Report generation logic
```

## ğŸ“š Documentation

- **[Deployment Guide](deployment_guide.md)**: Step-by-step instructions for deploying to AWS EC2.
- **[Scheduling Guide](scheduling.md)**: How to set up automated daily runs.

---

